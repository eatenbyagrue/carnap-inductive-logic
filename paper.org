#+LATEX_HEADER: \usepackage[backend=biber,authordate, ibidtracker=context,natbib,doi=false,isbn=false,url=false]{biblatex-chicago}
#+LATEX_HEADER: \usepackage{setspace}
# Pretty fractions
#+LATEX_HEADER: \usepackage{xfrac}
# Large circles
#+LATEX_HEADER: \usepackage{fdsymbol}
#+LATEX_HEADER: \usepackage{tikz}
# Fraktur Fonts
#+LATEX_HEADER: \usepackage{yfonts}
#+LATEX_HEADER: \addbibresource{~/Documents/bibliography/references.bib}
#+LATEX_HEADER: \usetikzlibrary{bayesnet}
#+LATEX_HEADER: \onehalfspacing
#+LATEX_HEADER: \newcommand{\Z}{\textfrak{Z}}
#+LATEX_HEADER: \renewcommand{\c}{\textfrak{c}}
#+LATEX_HEADER: \newcommand{\m}{\textfrak{m}}
#+LATEX_HEADER: \renewcommand{\L}{\textfrak{L}}
#+LATEX_HEADER: \newcommand{\Str}{\textfrak{Str}}

#+LATEX_HEADER: \newcommand{\wc}{\(\largecircle\)}
#+LATEX_HEADER: \newcommand{\bc}{\(\largeblackcircle\)}

#+LATEX_HEADER: \newcommand{\LFp}[1]{\citep[p.~#1]{carnap50_logic_found_probab}}
#+LATEX_HEADER: \newcommand{\LFt}[1]{\citet[p.~#1]{carnap50_logic_found_probab}}
#+LATEX_HEADER: \author{Conrad Friedrich}
#+OPTIONS: toc:nil num:t author:nil subtitle:nil
#+TITLE: Carnap's Early Inductive Logic
#+SUBTITLE: XxXx words
#+DATE: \today
\thispagestyle{empty}

\newpage
\tableofcontents
\newpage

In this paper we want to examine Rudolf Carnap's early attempts at solving the old problem of induction, famously posed by David Hume. 
* Introduction

- Describe the problem of induction
- What is induction?
- Carnap's approach: explicate probability
- talk about Carnap's concept of probability 
- But argue that in the end (of what is surveyed in this article), problem of induction is not solved, but logical probability in particular is explicated by highlighting reasonable biases
- Carnap on induction: p. 178ff.
- Second aim of this paper is to expound the structure of the formalism and explain which principles and assumptions lead to which conclusions. 
- Which cases are we looking at? distinguish between enumerative inductivion, generalized induction etc (e.g. see Springier)
- Do talk a bit about carnap's framework-choice stuff and how he thinks this relates to inductive logic: IL is a part of a framework, hence is defined for a particular language


Induction as described in this paper always takes the form of an inference from something observed to something unobserved. Both the observed and the unobserved are expressed in formal sentences in citet:carnap50_logic_found_probab or with the help of set theory in more run-of-the-mill probability theory.

* Developing a Framework for Confirmation  

** Carnap's Inductive System

Carnap starts out to explicate the concept of probability_1 by developing a first order language over which functions are defined. Eventually, the functions should assign probabilities to sentences. To develop a framework where he could achieve this, Carnap first defines a logical language. 

The language \L_\infty differs from \L_N by allowing for infinitely many individual constants, whereas \L_N only allows for /N/ many. Throughout this paper, we will only look at \L_N. Carnap postulates a one-to-one correspondence between individuals of the domain and individual constants \LFp{73}. Both languages only contain finitely many predicates.  

For our present purposes, \L_N is equivalent to standard languages of first order logic. There are some kinks and curiosities, but they do not impact any of the points made here. It's noteworthy that Carnap takes a semantically (as opposed to syntactically) oriented approach to defining his language, dovetailing nicely with the central role that state descriptions play in his formal system \LFp{vii}. He effectively limits the language to unary predicates to simplify the inductive rules. citet:paris15_pure_induc_logic expand Carnap's inductive system to /n/-ary predicates.

-maybe include a very short summary of FOL

Carnap pays special attention to /state descriptions/ \LFp{72}. A state description is a sentence which decides every predicate for every individual in the domain. That is, a state description conjoins each atomic sentence or its negation. Carnap labels these `\Z'. We'll adopt some of his notation to highlight when we are speaking of Carnap's system. 

The range \textfrak{R}_i of a sentence /i/ is the set of state descriptions in which /i/ holds. To give an inexact analogy: From a modern perspective we may view the state descriptions as models or possible worlds and ranges then as propositions. 

/Structure descriptions/ (labeled \Str) are also central to Carnap's inductive system. Roughly, a structure description is a set of state descriptions which share the number of instantiations of each predicate. The state descriptions in a structure description differ in that different individual constants instantiate the predicates. 

- Example 1 :: A language \L_3 with single predicate /P/ and three individual constants /a,b,c/. These three state descriptions form a structure description, as they share two that two individual instantiate the predicate /P/.
\begin{align*}
  P(a).P(b).\sim P(c) \\
  P(a).\sim P(b).P(c) \\
  \sim P(a).P(b).P(c) \\
\end{align*}  

Where `.' denotes a conjunction and `\sim' denotes a negation in Carnap's 
notation. In Carnap's system, the \Str{} are defined over isomorphic \Z. Two state descriptions are isomorphic when there is a correlation---a one-to-one mapping---between individual constants \LFp{109}, [[citep:caruspt_rudol_carnap][p. 8]]., citep:carus_rudol_carnap_carnap_induc_logic 

With the underlying language described, Carnap defines his confirmation function for sentences /h,e/, expressing the degree to which evidence /e/ confirms hypothesis /h/. He does so by first defining a measure function \m{} for sentences of \L_N \LFp{295} to the unit interval. Then he defines a confirmation function 

\begin{equation}
  \label{eq:conf} 
  \c(h,e) =_{df} \frac{\m(h.e)}{\m(e)}
\end{equation}

whenever \m(e) > 0. There are different equivalent axiomatisations in the literature, we'll give the one by [[citet:sznajder17_induc_logic_concep_spaces][p. 34]]:

\begin{align}
  \c(h,e) &\geq 0 \tag{C1} \\
  \c(e,e) &= 1 \tag{C2} \\
  \c(h,e) + \c(\sim h,e) &= 1 \tag{C3} \\
  \c(h.h',e) &= \c(h,e) \c(h',h.e) \text{ if } \m(h,e) > 0 \tag{C4} 
\end{align}

An additional requirement Carnap imposes right from the beginning is that his confirmation functions are /regular/. Regularity implies that for all state descriptions \( \m(\Z_i) > 0\), such that only logical falsehoods are measured zero.

\[
\c  \text{ is regular} \tag{C5}
\]

If we are interested in the unconditional confirmation, we condition on a logical tautology /t/:
\[
\c(e) =_{df} \c(e,t) = \frac{\m(e,t)}{\m(t)} = m(e).
\]


- Signpost :: These are the basic definitions for Carnap's confirmation function. In order to posit inductive rules, however, these axioms here are not sufficient. In fact, they merely state a conditional probability function, which does not impose any inductive constraints other than probabilistic consistency on the reasoner. Carnap, of course, wants to say more about these constraints. In section XXX we will examine which additional postulates lead to which normative consequences for the inductive reasoner. It is helpful to view the same constraints in the different, but for our purposes equivalent formal system of mathematical probability theory. That's why we'll introduce the relevant notions in the next chapter before returning to Carnap's inductive rules. 

** Basic Probability Theory

Following the more orthodox probability theory, we define a probability space \(\langle \Omega, \mathcal{F}, \Pr\rangle \), where: 

- \Omega is a set of outcomes of a hypothetical random experiment.
- \(\mathcal{F}\) is the set of relevant events. For finite \Omega, we can just include all possible events by requiring \(\mathcal{F} = 2^\Omega\), the power set.
- \( \Pr: \mathcal{F} \rightarrow [0,1] \) adheres to the following well-known axioms. Let \( H, B \in \mathcal{F} \), then
  \begin{align}
    \Pr(H) &\geq 0 \tag{P1}\\
    \Pr(\Omega) &= 1 \tag{P2}\\
    \Pr(H \cup E) &= \Pr(H) + \Pr(E) \tag{P3} \text{ for } H \cap E = \emptyset
  \end{align}

\Pr is then called a probability function. Note that we are not requiring \sigma-additivity, which also states P3 up to countably infinite union. This is analogous to just looking at Carnap's finite \L_N.

We define conditional probabilities in the standard way by 
\[
\Pr(H|E) = \frac{\Pr(H\cap E)}{\Pr(E)} \text{, if } \Pr(E) > 0. \tag{P4}
\]

Now we can also require regularity for \Pr, that is, 
\begin{align}
\Pr(\omega)>0  & \text{ for all }  \omega \in \Omega. \tag{P5}
\end{align}

One more bit of formalism is helpful for the following discussion. We are primarily interested in cases where we made a sequence of observations and want to estimate the next outcome. We can model this sequence and the following observation by a particular way of partitioning the sample space \Omega, with the help of random variables. A random variable for our purposes is a function \( X: \Omega \rightarrow \{1,\dots,t\} \). The random variable /X_i/ tracks the /i/-th observation with a number from 1 to /t/, so that the sequence /X_1/, \dots, /X_N/ stands for a sequence of /N/ observations. 
Additionally, following cite:zabell09_carnap_logic_induc_infer, we describe the frequencies of the outcomes, for /t/ possible outcomes of /X/, with a sequence \( n_1,\dots,n_t \). Probabilities for values of random variables are straightforwardly defined over the outcomes the random variables map from to those values.

** Parallels of Both Approaches

What is the purpose of making the reader sit through a reiteration of the standard axioms? We want to show the close analogy between both the Carnapian approach of defining a confirmation function as explication for probability_1 over a logical language and the standard mathematical way of defining a probability function. In particular, citet:zabell09_carnap_logic_induc_infer analyzes Carnap's approach exclusively from the perspective of mathematical probability. In keeping both approaches side by side, we make the parallels especially apparent. 
 
What are these parallels, then? Let us look at a simple predictive inference. In \L_N, we could describe consecutive observations as a series of individual constants and the predicates they instantiate. Continuing Example 1, we might observe individual /a/ first and determine all its properties, here, deciding whether predicate /P/ applies or not. Next, we observe individual /b/ and so on. A state description is then a complete description of all observations. 

We could also describe the same consecutive observations with probability theory, by assigning a random variable \(X_1\) to the first observations, \(X_2\) to the second and so on. A sequence \(X_1 = 1,X_2 = 1,X_3=0\) would then describe the same observations as the first state description in Example 1. A sequence like this is usually used to describe the outcome of a random process, additionally requiring the random variables to be independent and identically distributed. That is, they are assumed to be sampled from the same underlying distribution and independently so, conditional on the (unknown) parameters of that distribution. We do /not/ require that here, however. 

The frequencies of the values in Example 1 can be written as the sequence \(n_1=1,n_2=2\), indicating that a `0' has occurred one time, and a `1' has occurred two times. For all three state descriptions, the frequency counts are identical. For different structure descriptions, the frequency counts differ. Hence, we can represent Carnap's structure descriptions by the sequences of frequency counts.

Quite strikingly, Carnap's measure function \m{} resembles unconditional probabilities, where the confirmation function \c resembles conditional probabilities. More so, it can be shown that both are equivalent, given a suitable corresponding assignment of sentences and random variables, much like we proposed just now.

* Constraints to the Confirmation Function

- Signpost :: Here goes some signposting, talk about that c function not determined uniquely.

** Symmetry   

Carnap defines a symmetric \m function: If \(\Z_i\) and \(\Z_j\) are isomorphic, then \( \m(\Z_i)~=~\m(\Z_j)\). All state descriptions of a structure description are assigned the same measure. A symmetric confirmation function is then defined just as before by conditionalizing. If we require for a confirmation function that 

\begin{equation}
\tag{C5} \c \text{ is symmetric}
\end{equation}

we receive a confirmation function which assigns equal confirmation to all state descriptions. This means that to determine the confirmation of a state description, it is sufficient to determine the confirmation of the corresponding structure description. Carnap also calls this feature `The principle of invariance' \LFp{489}, since the confirmation is invariant under permutation of the individual constant, while keeping the predicates fixed. 

He argues that this principle is tacitly agreed on by many authors involved in, using his terminology, explicating probability_1 \LFp{488}. His argument for this principle is straight-forward: On purely logical grounds, we do not have any more reason to expect a particular individual to have a certain property than any other individual. Hence a logical confirmation function does not distinguish these cases. As cite:caruspt_rudol_carnap note, subjective Bayesians would not agree with this assessment. Although they might agree about the specifics of the observations, they also allow non-symmetric[fn::We use /non-symmetric/ instead of /asymmetric/ to emphasize the negation of the specific sense in which symmetry is used here.] prior probability functions. 

The requirement of symmetry already appeared in cite:johnson24_logic_part_iii under the name `permutation postulate'. Interestingly, Carnap cites Johnson [[citep:caruspt_rudol_carnap][p. 10]], but is apparently not aware of his result [[citep:caruspt_rudol_carnap][p. 8]]. 

Symmetry alone does not yield a unique confirmation function, as we'll discuss next.

** The Function c\dag 

The most apparent function that respects symmetry assigns the same value to all state descriptions. By assigning the same confirmation to all state descriptions, we of course also assign the same confirmation to all state descriptions of a structure description. Carnap calls this function \c\dag. He immediately notes the utter uselessness of \c\dag for the purpose of inductive logic, since it makes learning impossible \LFp{565}. In his example we look at a language \L_{101} with a single predicate, where we have already observed that \(Pa_1, Pa_2, \dots, Pa_{100}\). Consider \( h = Pa_{101} \). Then \( \c\dag(h,e) = \frac{\m(h.e)}{\m(e)}\), and since \(\m(e) = \m(h.e) + \m(\sim h.e)\) we have \c\dag(h,e) = 1/2. Although /all/ other individuals where /P/, the confirmation function did not learn anything. This is obviously undesirable.

The degree of confirmation \c\dag assigns to \Z_i is then just dependent on the number of state descriptions \(\zeta = \kappa^N\), where /N/ is the number of individual constants in \L_N and \kappa refers to the number of Q-predicates (quasi-predicates). These, roughly, give the different ways in which an individual can be described in \L_N. For example, for a language with a single predicate there are two Q-predicates. Then we can determine 
\[
  \m(\Z_i) = \zeta^{-1}.
\]

The function \c\dag is then defined over \m\dag as described in Equation \ref{eq:conf}. But the function \c\dag shows more, namely that merely respecting the symmetry requirement does not suffice to ensure the /principle of positive instantial relevance/ (PPIR), described by citet:humburg71_princ_instan_relev, which is a central inductive tenet. The PPIR states that for any evidence /e/, individual constants /a,b/, predicate /P/:
\[
\tag{PPIR} \c(Pa,e.Pb) > \c(Pa,e).
\]
That is, observing another instance should strictly increase the degree of confirmation. This is not the case with \c\dag, which satisfies symmetry, hence symmetry does not suffice for PPIR.

** Structure Description Symmetry

In the appendix of his Logical Foundations, Carnap proposes an additional constraint on measure functions: Assign equal confirmation to all /structure descriptions/, too. In want of a label, we'll call this \Str{}-symmetry.

Carnap does not, in fact, give a positive reason to require \Str{}-symmetry, he even doubts that one might be given \LFp{564}. As we will see in sec X.X, this is the requirement which Carnap weakens when introducing his continua of inductive rules.

This requirement, too, was described earlier by W.E. Johnson citep:zabell82_w,zabell09_carnap_logic_induc_infer, which Johnson labeled 'the combination postulate'.

** The Function c*

Both requirements, symmetry and \Str{}-symmetry, taken together yield a unique confirmation function \c* \LFp{563f.}. Carnap first, again, defines \m*:
\[
\m\text{*}(\Z_i) =_{df} \frac{1}{\tau\zeta_i}
\]
where \tau is the number of structure descriptions in \L_N and \zeta_i the number of state descriptions isomorphic to \Z_i, that is, those which share a structure description with \Z_i. This definition is not arbitrary, instead it follows directly from both symmetry requirements. 

- maybe say a sentence more why it obviously follows. but will be addressed later anyway.

Carnap determines \tau \LFp{138}:
\begin{equation}
\tau = {N + \kappa - 1 \choose \kappa -1}
\end{equation}
where /N/, like before, is the number of individuals and \kappa the number of {Q-predicates} for \L_N, and he determines \zeta_i \LFp{140}:
\[
\zeta_i = {N \choose N_1,\dots,N_{\kappa}}
\]
where \( N_1,\dots,N_{\kappa} \) gives the number of individuals that instantiate each of the \kappa many Q-predicates in \Z_i. Carnap's terminology can be a bit cumbersome, which makes these definitions less obvious. To help clarify, we follow citet:carnap55_statis_induc_probab,caruspt_rudol_carnap and give the following simple example.

- Example 2 :: Let \L_4 be a language consisting of a single predicate /P/ and four individual constants /a,b,c,d/. Then we have \(\kappa = 2\) Q-predicates (just /P/ and \(\sim P\)), and therefore \(\zeta = \kappa^N = 16 \) state descriptions. We have \(\tau = {5 \choose 1}\) structure descriptions. \Str_3, whose state descriptions have two individuals that are /P/ and two that aren't, has \(\zeta_{6} = \dots = \zeta_{11} = {4 \choose 2} = 6 \) state descriptions. The corresponding measures are given in the overview table.

#+ATTR_LATEX: :float t :environment longtable 
| \Str_j | \Z_i | P(a) | P(b) | P(c) | P(d) | \m\dag(\Str_j) | \m\dag(\Z_i)  | \m*(\Str_j)  | \m*(\Z_i)     |
|--------+------+------+------+------+------+----------------+---------------+--------------+---------------|
|--------+------+------+------+------+------+----------------+---------------+--------------+---------------|
|      1 |    1 | \bc  | \bc  | \bc  | \bc  | \sfrac{1}{16}  | \sfrac{1}{16} | \sfrac{1}{5} | \sfrac{1}{5}  |
|--------+------+------+------+------+------+----------------+---------------+--------------+---------------|
|      2 |    2 | \bc  | \bc  | \bc  | \wc  | \sfrac{1}{4}   | \sfrac{1}{16} | \sfrac{1}{5} | \sfrac{1}{20} |
|        |    3 | \bc  | \bc  | \wc  | \bc  |                | \sfrac{1}{16} |              | \sfrac{1}{20} |
|        |    4 | \bc  | \wc  | \bc  | \bc  |                | \sfrac{1}{16} |              | \sfrac{1}{20} |
|        |    5 | \wc  | \bc  | \bc  | \bc  |                | \sfrac{1}{16} |              | \sfrac{1}{20} |
|--------+------+------+------+------+------+----------------+---------------+--------------+---------------|
|      3 |    6 | \bc  | \bc  | \wc  | \wc  | \sfrac{3}{8}   | \sfrac{1}{16} | \sfrac{1}{5} | \sfrac{1}{30} |
|        |    7 | \bc  | \wc  | \bc  | \wc  |                | \sfrac{1}{16} |              | \sfrac{1}{30} |
|        |    8 | \bc  | \wc  | \wc  | \bc  |                | \sfrac{1}{16} |              | \sfrac{1}{30} |
|        |    9 | \wc  | \bc  | \bc  | \wc  |                | \sfrac{1}{16} |              | \sfrac{1}{30} |
|        |   10 | \wc  | \bc  | \wc  | \bc  |                | \sfrac{1}{16} |              | \sfrac{1}{30} |
|        |   11 | \wc  | \wc  | \bc  | \bc  |                | \sfrac{1}{16} |              | \sfrac{1}{30} |
|--------+------+------+------+------+------+----------------+---------------+--------------+---------------|
|      4 |   12 | \bc  | \wc  | \wc  | \wc  | \sfrac{1}{4}   | \sfrac{1}{16} | \sfrac{1}{5} | \sfrac{1}{20} |
|        |   13 | \wc  | \bc  | \wc  | \wc  |                | \sfrac{1}{16} |              | \sfrac{1}{20} |
|        |   14 | \wc  | \wc  | \bc  | \wc  |                | \sfrac{1}{16} |              | \sfrac{1}{20} |
|        |   15 | \wc  | \wc  | \wc  | \bc  |                | \sfrac{1}{16} |              | \sfrac{1}{20} |
|--------+------+------+------+------+------+----------------+---------------+--------------+---------------|
|      5 |   16 | \wc  | \wc  | \wc  | \wc  | \sfrac{1}{16}  | \sfrac{1}{16} | \sfrac{1}{5} | \sfrac{1}{5}  |

 Since symmetry and \Str-symmetry determine a unique confirmation function, they also determine a unique inductive rule which prescribes how much any evidence confirms any hypothesis expressible in \L_N. In the section X, we will give a derivation of this rule and examine its properties. In order to do so and to elucidate the foregoing, it is very helpful to rephrase what has been developed in this section in terms of probability theory.

** Constraints Probability Theory

 Carnap's contraints and confirmation functions can be straightforwardly represented as standard probability. Recall from Section [[Parallels of Both Approaches]] that a state description as Carnap uses them can be analogously described as a sequence of random variables taking definite values. Each random variable then stands for an individual constants. The values of a random variable correspond to the Q-predicates. For example, a random variable \(X_i\) taking a value in \(1,\dots,t\) with \(t=4\) corresponds to an individual constant satisfying one of the \(\kappa = 4\) formulas \(P(x).R(x), ~P(x).\sim R(x), ~\sim P(x).R(x), ~\sim P(x).\sim R(x)\).[fn::As an aside, note that the number of Q-predicates used this way is constricted to powers of two, since each atomic predicate can either be instantiated or not. Using random variables is more expressive in this sense, since we also can set \(t=3\). In the preface to his second edition \LFp{xx}, Carnap notes that the same expressiveness can be achieved by giving up the requirement of logical independence of the atoms.] We follow cite:zabell09_carnap_logic_induc_infer in describing this approach in probability theory.  

We can require /symmetry/ on a probability function by assigning each sequence \(X_1 = e_1,\dots,X_n=e_n\) (in short: \(e_1,\dots,e_n\) of a frequency count \(n_1,\dots,n_t\) the same probability. This assumptions is also called /exchangeability/ since Bruno de Finetti [[citep:zabell09_carnap_logic_induc_infer][p. 272]]. Since the different sequences are mutually exclusive, this amounts by P3 to
\begin{equation}
\label{eq:probfreq1}
\Pr(n_1,\dots,n_t) = {n \choose n_1\dots n_t} \Pr(e_1,\dots,e_n)
\end{equation}

Carnap's \Str-symmetry can be captured by requiring the same probability for all frequency counts. The number of all frequency counts is given by the number of ordered t-partitions of n:
\begin{equation}
\label{eq:freq}
f_n = \binom{n + t - 1}{n}
\end{equation}
which is equal to Carnap's number of structure descriptions \tau[fn:: citet:zabell07_carnap,zabell09_carnap_logic_induc_infer gives this count as \({n +t -1 \choose t}\), citing cite:feller68_introd_probab_theor_applic, which we maybe mistakenly found to be not equivalent to Carnap's formulation and also to not yield the later results.] (see Appendix). The frequency count's probability then accordingly
\begin{equation}
\label{eq:probfreq2}
\Pr(n_1,\dotsc,n_t) = \frac{1}{f_n}.
\end{equation}

Equations \ref{eq:probfreq1}, \ref{eq:freq} and \ref{eq:probfreq2} together give
\begin{equation}
\Pr(e_1,\dotsc,e_n) = \left[ \binom{n+t-1}{n}\binom{n}{n_1\dots n_t}\right]^{-1}
\end{equation}



* Inductive Rules/Predictive Inference

** Carnap
- Follow cite:zabell09_carnap_logic_induc_infer in computing p = c* for random variables with more than one value. Fill in derivations where 'just a little algebra is required', but relegate the simplest algebra to the appendix.
- Then: Apply to example with different inductive inferences
** Probability Theory

* The \lambda-continuum

Describe shortly Carnap's \lambda -continuum: A parametric family of inductive rules. What do they enable? again using the example. Show effect of different lambdas in a graph

** The \lambda-continuum: probability Theory

Quote Skyrms, Zabell, that symmetric dirichlet distribution account for this in probability theory. maybe develop this for a simple example! -> beta distribution. see also Kruschke.

* What's next?

Give some ideas on where to go. obviously, \lambda -\gamma -continuum. Also: Frequencies of frequencies (noted by Turing, says Zabell, look into that a little).
* Appendix
** Equation 
\printbibliography
